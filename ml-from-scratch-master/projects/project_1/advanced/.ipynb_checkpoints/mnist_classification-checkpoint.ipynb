{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST Classification projects\n",
    "\n",
    "In this project, I'm assigned to do classification on the MNIST handwritten digit, this is an old dataset for any Machine Learning Beginner. \n",
    "\n",
    "At the moment, I've learnt KNN, Naive Bayes, Decision Tree and SVM (convert to dual problem and solve by quadratic programming). After the first try with SVM quadratic programming with the following objective function:\n",
    "\n",
    "$$ g(\\lambda, \\mu) = \\min_{\\mathbf{w}, b, \\xi} \\mathcal{L}(\\mathbf{w}, b, \\xi, \\lambda, \\mu) $$\n",
    "$$ g(\\lambda, \\mu) = \\sum_{n=1}^N \\lambda_n - \\frac{1}{2} \\sum_{n=1}^N\\sum_{m=1}^N \\lambda_n \\lambda_m y_n y_m k(\\mathbf{x}_n^T, \\mathbf{x}_m) $$\n",
    "\n",
    "\\begin{eqnarray}\n",
    "     \\lambda &=& \\arg \\max_{\\lambda} g(\\lambda)   &&\\\\\n",
    "     \\text{subject to:}~ && \\sum_{n=1}^N \\lambda_ny_n = 0 && \\\\\n",
    "     && 0 \\leq \\lambda_n \\leq C, ~\\forall n= 1, 2, \\dots, N && \n",
    "\\end{eqnarray}\n",
    "\n",
    "I realize that it's impossible to compute $ k(\\mathbf{x}_n^T, \\mathbf{x}_m) $ for the number of dataset = 60000. One way to overcome this difficulty is that approach the problem in another way in SVM that we can use gradient descent or iterative methods and change the loss function, I will try this approach later.\n",
    "\n",
    "In this project, I do 2 algorithms in deep learning way: \n",
    "- Softmax Regression (no hidden layer)\n",
    "- Lenet (5 hiddens layer)\n",
    "\n",
    "--------------\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Load MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------> Downloading MNIST dataset\n",
      "-------> Finish\n"
     ]
    }
   ],
   "source": [
    "from utils import load_dataset_mnist, preprocess_data\n",
    "from mnist import MNIST\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "load_dataset_mnist()\n",
    "\n",
    "mndata = MNIST('data_mnist')\n",
    "\n",
    "images, labels = mndata.load_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Training or Loading Lenet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/giangtran/Desktop/framgia-training/venv/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:642: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "----> LOADED WEIGHTS\n",
      "Accuracy: 0.9863\n",
      "Confusion matrix: \n",
      "[[ 970    0    1    0    1    0    5    1    2    0]\n",
      " [   0 1129    2    0    0    2    2    0    0    0]\n",
      " [   2    2 1020    1    1    0    0    4    2    0]\n",
      " [   0    0    1  996    0    4    0    5    2    2]\n",
      " [   0    0    1    0  966    0    4    1    1    9]\n",
      " [   3    0    0    5    0  881    2    1    0    0]\n",
      " [   0    2    1    0    3    2  948    0    1    1]\n",
      " [   0    6    4    0    1    0    0 1010    1    6]\n",
      " [   1    0    2    3    1    5    0    2  956    4]\n",
      " [   2    2    0    1    7    4    0    3    3  987]]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from lenet import Lenet\n",
    "import tensorflow as tf\n",
    "\n",
    "training_phase = \"saved_model\" not in os.listdir()\n",
    "\n",
    "lenet = Lenet(20, 64, tf.train.AdamOptimizer(learning_rate=0.001), tf.losses.softmax_cross_entropy)\n",
    "\n",
    "if training_phase:\n",
    "    images, labels = mndata.load_training()\n",
    "    images, labels = preprocess_data(images, labels, True)\n",
    "    lenet.train(images, labels)\n",
    "else:\n",
    "    images_test, labels_test = mndata.load_testing()\n",
    "    images_test, labels_test = preprocess_data(images_test, labels_test, True, True)\n",
    "    lenet.load_model()\n",
    "    pred = lenet.predict(images_test)\n",
    "    print(\"Accuracy:\", len(labels_test[pred == labels_test]) / len(labels_test))  # 98%\n",
    "\n",
    "    from sklearn.metrics.classification import confusion_matrix\n",
    "    print(\"Confusion matrix: \")\n",
    "    print(confusion_matrix(labels_test, pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------\n",
    "Accuracy is 98% with 20 epochs. Very well!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Training with Softmax Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/giangtran/Desktop/framgia-training/venv/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:368: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at epoch 1 5.61\n",
      "Loss at epoch 2 2.89\n",
      "Loss at epoch 3 2.09\n",
      "Loss at epoch 4 1.72\n",
      "Loss at epoch 5 1.50\n",
      "Loss at epoch 6 1.35\n",
      "Loss at epoch 7 1.24\n",
      "Loss at epoch 8 1.16\n",
      "Loss at epoch 9 1.09\n",
      "Loss at epoch 10 1.04\n",
      "Loss at epoch 11 1.00\n",
      "Loss at epoch 12 0.96\n",
      "Loss at epoch 13 0.92\n",
      "Loss at epoch 14 0.89\n",
      "Loss at epoch 15 0.87\n",
      "Loss at epoch 16 0.84\n",
      "Loss at epoch 17 0.82\n",
      "Loss at epoch 18 0.80\n",
      "Loss at epoch 19 0.79\n",
      "Loss at epoch 20 0.77\n",
      "Accuracy: 0.8472\n",
      "Confusion matrix: \n",
      "[[ 910    1    8    6    0   22   13    4   13    3]\n",
      " [   0 1070    4   10    0    5    3    2   40    1]\n",
      " [  14    9  865   25   14    6   20   18   50   11]\n",
      " [   9    7   28  841    0   61    3   11   34   16]\n",
      " [   1    5   11    5  836    9   26    9   20   60]\n",
      " [  17    7   11   71   23  643   23   10   77   10]\n",
      " [  30    3   16    4   24   14  854    1   10    2]\n",
      " [   9   14   28   10   10    2    3  878   13   61]\n",
      " [  13   14   20   41   19   51   15   17  764   20]\n",
      " [  11    5    4    9   74   15    2   53   25  811]]\n"
     ]
    }
   ],
   "source": [
    "from softmax_regression import SoftmaxRegression\n",
    "\n",
    "images, labels = mndata.load_training()\n",
    "images, labels = preprocess_data(images, labels)\n",
    "softmax = SoftmaxRegression(epochs=20)\n",
    "softmax.train(images, labels)\n",
    "\n",
    "images_test, labels_test = mndata.load_testing()\n",
    "images_test, labels_test = preprocess_data(images_test, labels_test, test=True)\n",
    "\n",
    "pred = softmax.predict(images_test)\n",
    "\n",
    "print(\"Accuracy:\", len(pred[labels_test == pred]) / len(pred))\n",
    "from sklearn.metrics.classification import confusion_matrix\n",
    "print(\"Confusion matrix: \")\n",
    "print(confusion_matrix(labels_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With softmax regression the accuracy is 85%! Not so bad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
